\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage{setspace}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{indentfirst}
\bibliographystyle{plainnat}
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}

%\usepackage{bera}% optional: just to have a nice mono-spaced font
\usepackage{listings}
\usepackage{xcolor}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\begin{document}
\title{Research Proposal}
\author{Andrew Valencik}
\date{\today}
\maketitle
\tableofcontents
\begin{doublespacing}

%What is the problem? 
%Why should it be done? 
%Where will it be done? 
%How will you be doing it?
%How much will it cost? 
%How long will it take? 

\pagebreak
\section{Abstract}
%The problem and its setting
%Proposed methodology to solve problem

\pagebreak
\section{Introduction}
% 5 pages double spaced
% A lot of this can got taken from literature review
% Define reseach setting and sum-up what has been done
% Point towards what should be done, why?

%What is the problem? What will I study?
The academic field of nuclear science is over one hundred years old, starting with the discovery of radiation.
This discovery is the first of many entries in the Nuclear Science References database, collected, catalogoued, distributed, and evaluated by the National Nuclear Data Center. \citep{Kurgan200603} 
The NSR has over 210,000 entries documenting the body of nuclear science literature, which provides the opportunity for knowledge discovery on the literature's meta data.
This work is a cross disciplinary work, combining data mining technique with domain knowledge in nuclear physics. 


\subsection{Motivation}
%Why is it an important question?
The current search facilities of the NSR are limited to text matching against the NSR entries.
This informatoin could be of more value to researchers if more complex queries could be run on the data.

This process should reveal trends in the collective scientific study of nuclear structure, processes, and detection.
Additionally, categorizing trends may provide predictive power in determining a worthwhile area of study or application of a technique.
We want to reveal patterns and understand connections in the body of nuclear science literature to predict worthwhile areas of research.

To find unexplored areas.
To quantify the growth in areas of research over time.
To search for interesting structures in the body of literature. The structure of citations and contributions may provide interesting insight.

To apply social mining tools to an academic dataset.
To discover.


\subsection{Review}
%What do we know already?
Information retrieval has been repeatedly improved by large search engines like Google.
And while nuclear science literature is a specialized field, Google Scholar is capable of searching many peer reviewed journal articles.

Page rank algorithm.

%How will this advance our knowledge?
The NSR provides metadata that can be converted to semantic information for advanced querying capabilities.

We can potentially use citation data as a proxy for PageRank

%Explain your choice of model. Advantages? Suitability from theoretical point of view? practical reasons for using it?

%Significance, timeliness, and importance of project.

\pagebreak
\section{Objectives}
% 1-2 pages double spaced
%Define specific goals of research (which part of general problem am I tackling?)
%Expected outcome
Determine the structure of the nuclear science literature.
Quantify the areas of growth and contribution over time.

\pagebreak
\section{Methodology}
% 5 pages double spaced. Most critical part. ~70 sentences
%Critical. Define plan of action.
%How to tackle problem? Why this way?

%Creating the DB
\subsection{Data Preparation}
The most preliminary step is acquiring the Nuclear Science References from the National Nuclear Data Center.
A full database dump was acquired on January 29th 2014.
This dump of the NSR data will hereinafter be reffered to as if it were the complete NSR database.
All efforts will be taken to ensure the research procedures can very easily be extended and repeated on new NSR data.
This is discussed in further detail in section \ref{sustainability}.

%Data Cleaning
Before the data can be imported into MongoDB it must be parsed into a JSON format.
The NSR data is provided in a custom EXCHANGE format. \citep{NSRhandbook}
An example of the raw data for a single paper can be seen in figure !!! %%%%ADD FIGURE
A series of simple search and replace commands using regular expressions can be applied to transform the data into a valid JSON structures.
The JSON structure for the entry in figure \ref{exNSR} is shown in \ref{exJSON}. %Why JSON?

In order to produce a good JSON schema, thought must be given to the data representation.
Consideration should be given to the types of queries that will be made on the data.
For example the author field is likely be represented as an array of strings, which each unique author being a separate element in the array.
This attaches information about how many authors an entry has to our schema.

%Data Representation
The optimal schema for the SELECTRS field is not initially obvious.
The current schema has SELECTRS parsed into a 3 dimensional array.
Each 'selector' has a type, value, and a link variable.
The following types are valid:
\begin{quote}
N, T, P, G, R, S, M, D, C, X, A, or Z, which stand for nuclide, target, parent, daughter, reaction, subject, measured, deduced, calculated, other subject, mass range, and charge range, respectively.
\end{quote}
The value changes based on the type. The link variable is used to tie together multiple selectors.

\subsection{Data Summarization}
%Data Summarization
Data summarization is necessary to begin to understand our data.
Preliminary visualizations help in this task.
A histogram of database entries per year is shown in figure \ref{fig:NSRyearly}.
This very quickly demonstrates that the majority of documents in the NSR were published in the last 50 years.

\begin{figure}[!hb]
    \label{fig:NSRyearly}
    \centering
    \includegraphics[width=\linewidth]{NSRyearly.pdf}
    \caption{NSR Database documents per year from 1896 to 2014.}
\end{figure}

%Form data mining problems, questions, queries
%Interpretation

\subsection{Classification and Cluster Analysis}
Classification and clustering are related approaches to organizing data elements into groups for further analysis.
Classification is the process of deciding what group a particular datum should most optimally belong to.
Clustering is the grouping of multiple data points such that those belonging to a group are more similar in some manner than those outside of that group.

Clustering can be broken into two main groups, hierarchical and partitional.
Both groups have applications in this study.
The citation structure of the literature is likely to be hierarchical in nature.
Partitional clustering should prove useful in determining the sub genres and fields of study within the body of work.

K-means clustering is a cluster analysis technique that can group data objects in $K$ clusters based on minimizes their distances with respect to cluster centroids.
K-means is a partitional clustering algorithm.
In text mining or document comparing we often use cosine similarity between term frequency vectors

\subsubsection{K-means Clustering}
K-means clustering is a cluster analysis technique that can group data objects in $K$ clusters based on minimizing their distances with respect to cluster centroids.
K-means is a partitional clustering algorithm.

Say we have a finite set of objects,  $X = {x_1, x_2, ..., x_n}$ where each is a data object in $d$ dimensions.
We can create $k$ clusters $C = {c_1, c_2, ..., c_k}$ where $k <= n$.
The process starts by randomly choosing $k$ points, ${x_1, ..., x_k}$ to be the centroids of a cluster.
Iterate over each object $x$ and assign it to a cluster $c$ based on the minimization of some parameter, for now, Euclidean distance.
The new centroids are now computed and the process is repeated until cluster stability is achieved.
The goal is to minimize the total sum of squared errors between the centroids and all objects. 
\begin{equation} \label{kmeans}
J(C) = \sum^K_{k=1} \sum_{x_i \in c_k} \left| x_i - \mu_k \right| ^2
\end{equation}

\begin{figure}[!hb]
    \centering
    \includegraphics[width=\linewidth]{../literatureReview/kmeansJain.pdf}
    \caption{Step by step illustration of K-means algorithm. (a) The initial input data. (b) Three seed points are chosen as the starting 'centroids' and the data points are assigned to the cluster with the closest seed point. (c) (d) The centroids of the new clusters are calculated, and data labels updated; (e) the iteration stops when the clusters converge.}
    \label{fig:K-Means-Jain}
\end{figure}

Three parameters for K-means must be specified initially, the number of clusters, initial centroid guesses, and the distance metric.
The metric is the function on a space that describes how two points differ from one another, i.e. distance.
Euclidean distance is typically used, leading to ball or sphrer shaped clusters. \citep{Jain2010651}
Other metrics maye have different results, like the Mahalanobis distance metric which leads to hyperellopsoidal clusters. \citep{MaoJain1996}
The choice of metric may be domain specific; the Itakura-Saito distance represents the error between an auditory spectrum and its approximate, it is used in speech processing. \citep{chan2008advances}

The chosen number of clusters has a huge impact on the data partitions.
Some heuristics exist to aid in determining an optimal $k$. \citep{tibshirani2001estimating}
However, in practice, K-means is normally run multiple times with varying $k$ values and the best is selected by a domain expert.



\subsection{Text Mining}
Text mining is an area of analysis that focuses on extracting useful information from unstructured plain text data.
The data to analyze is often natural language text written by humans.
Examples of such data include user reviews of a product or service, customer feedback comments, emails, forum posts, or even academic journal articles.

A simple goal of text mining could be to summarize the input text.
However, this can be abstracted further to 'numericizing' text data for use in an analysis model.

\subsubsection{Cosine Similarity}
%https://stackoverflow.com/questions/15173225/how-to-calculate-cosine-similarity-given-2-sentence-strings-python
The K-means discussion above mentioned a few types of distance metrics for use in clustering.
When dealing with text we often use the cosine similarity of two documents to compute their 'distance.
We build a term vector for each document to be analyzed, this is a vector describing the amount of occurrences each word has in the document
Say document one, $d1$, is "The quick brown fox jumped over the lazy dog" and document two, $d2$, is "The brown dog jumped over the brown fox".
We create a term vector for each document, as seen in table \ref{termtable}. 
Often in doing this, very common words like 'the' are omitted.
Equations \ref{d1termvector} and \ref{d2termvector} are the term vectors for each document.
The cosine similarity is the dot product of the two documents divided by the product of their magnitudes (\ref{cosinesimilarity}). \citep{huang2008similarity}

\begin{table} \label{termtable}
    \begin{tabular}{llllllll}
    ~  & quick & brown & fox & jumped & over & lazy & dog \\
    d1 & 1     & 1     & 1   & 1      & 1    & 1    & 1   \\
    d2 & 0     & 2     & 1   & 1      & 1    & 0    & 1   \\
    \end{tabular}
\end{table}

\begin{equation} \label{d1termvector}
d1 = \left< 1, 1, 1, 1, 1, 1, 1 \right>
\end{equation}

\begin{equation} \label{d2termvector}
d2 = \left< 0, 2, 1, 1, 1, 0, 1 \right>
\end{equation}

\begin{equation} \label{cosinesimilarity}
\cos (d1,d1) = \frac{d1 \cdot d2 }{\left| d1 \right| \left| d2 \right|} 
\end{equation}

\begin{equation} \label{cosinesimilarity1}
 d1 \cdot d2 = (1)(0) + (1)(2) + (1)(1) + (1)(1) + (1)(1) + (1)(0) + (1)(1) = 6.0 
\end{equation}

\begin{equation} \label{cosinesimilarity2}
 \left| d1 \right| = \left( (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 \right) ^{\frac{1}{2}} =  2.645751311
\end{equation}

\begin{equation} \label{cosinesimilarity3}
 \left| d2 \right| = \left( (0)^2 + (2)^2 + (1)^2 + (1)^2 + (1)^2 + (0)^2 + (1)^2 \right) ^{\frac{1}{2}} =  2.828427125
\end{equation}

\begin{equation} \label{cosinesimilarityF}
\cos (d1,d2) = \frac{d1 \cdot d2 }{\left| d1 \right| \left| d2 \right|} = \frac{6.0}{\left| 2.645751311 \right| \left| 2.828427125 \right|} = 0.8017837257
\end{equation}

As equation \ref{cosinesimilarityF} shows, the two documents are quite similar, and thus have a high cosine similarity.
When $d1 = d2$ the similarity is 1.0.
This technique is a simple way of numercizing text for further mathematical manipulation and treatment.



\subsection{Sustainability} \label{sustainability}
The analysis and visualizations produced for this work will be constantly updated.
New data can be acquired from the NNDC and uploaded to the local database with relative ease.


\pagebreak
\section{Equipment and Research Materials}
% 1 pages double spaced
%Demonstrate feasibility

%Computing resources, hosting, database.
The computing resources for this project will not be substaintial.
It is likely that all database queries will be able to complete in a timely manner on a MacBook Pro.

The work will be hosted on equipment owned by the Mathematics and Computing Science department, and housed in the ITSS server room.

\pagebreak
\section{Budget}
% 1 pages double spaced
%Breakdown of expected expenses.
The largest expense of this project would be in producing the original database.
Thankfully the researchers at the NNDC have already done so and willing shared a copy.

The computational and hosting requirements for this work, as outlined above, will be met by existing Saint Mary's resources.
All software used in the completion of the work will be open source.

\pagebreak
\section{Timetable}
% 1 pages double spaced
%Expected date of completion for each major step in research project
%Realistic for 2 year project

%%% That's all folks
\pagebreak
\end{doublespacing}

\bibliography{../citationsAPSC.bib}

\end{document}
