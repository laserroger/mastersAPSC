\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage{setspace}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{indentfirst}
\bibliographystyle{plainnat}
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}

%\usepackage{bera}% optional: just to have a nice mono-spaced font
\usepackage{listings}
\usepackage{xcolor}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\footnotesize\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\begin{document}
\title{Research Proposal}
\author{Andrew Valencik}
\date{March 26, 2014}
\maketitle
\tableofcontents
\listoffigures
\begin{doublespacing}

%What is the problem? 
%Why should it be done? 
%Where will it be done? 
%How will you be doing it?
%How much will it cost? 
%How long will it take? 

\pagebreak
\section{Abstract}
The National Nuclear Data Center prepares an evaluated database of nuclear science literature that poses a rich opportunity for knowledge discovery directed at the scientific work and study. 
Identifying trends and patterns in the meta data of over 200,000 documents may reveal latent structures of the progress and collaboration among the scientific community. 
This knowledge may have implications in other similar scientific fields, but most prominently it may provide predictive power within the nuclear science domain.

All processing is done on a local copy of the Nuclear Science References database.
At the present stage the data has been cleaned, formatted, and 212835 documents have been imported to a MongoDB database. 
Preliminary visualizations and data summarization efforts are visible at http://pig.io.
Current efforts are to finalize the data schema, and build a platform for handling objects created as the result of database queries.

The methodology and analysis will, whenever possible, use modern web development practices with the goal of having a publicly accessible tool as the outcome. 
This is a cross-disciplinary endeavor between physics and computing science. 
The applied techniques and tool set will draw largely from the knowledge discovery and data mining fields of computer science. 
Nuclear physics represents the domain knowledge of the data, and will thusly make up a large portion of discussion and research.
%The problem and its setting
%Proposed methodology to solve problem

\pagebreak
\section{Introduction}
% 5 pages double spaced
% A lot of this can got taken from literature review
% Define research setting and sum-up what has been done
% Point towards what should be done, why?

%What is the problem? What will I study?
The academic field of nuclear science is over one hundred years old, starting with the discovery of radiation.
This discovery is the first of many entries in the Nuclear Science References database, collected, cataloged, distributed, and evaluated by the National Nuclear Data Center. \citep{Kurgan200603} 
The NSR has over 210,000 entries documenting the body of nuclear science literature, which provides the opportunity for knowledge discovery on the literature's meta data.

%What do we know already?
%How will this advance our knowledge?
%Explain your choice of model. Advantages? Suitability from theoretical point of view? practical reasons for using it?
%Significance, timeliness, and importance of project.
Information retrieval has been repeatedly improved by large search engines like Google.
And while nuclear science literature is a specialized field, Google Scholar is capable of searching many peer reviewed journal articles.
The NSR provides meta data that can be converted to semantic information for advanced querying capabilities.
This work is a cross disciplinary work, combining data mining technique with domain knowledge in nuclear physics. 

\subsection{Motivation}
%Why is it an important question?
The current search facilities of the Nuclear Science References are limited to text matching against the NSR entries.
This information could be of more value to researchers if more complex queries could be run on the data.

The knowledge discovery and data mining process should reveal trends in the collective scientific study of nuclear structure, processes, and detection.
Additionally, categorizing trends may provide predictive power in determining a worthwhile area of study or application of a technique.
The analysis outcomes of studying literature meta data may provide useful when analyzing bodies of work from other disciplines.

This process will enable the quantification of growth in areas of research over time, as well as the contributions of authors or collaborations.
These types of analytics will be useful in summarizing what the field of nuclear science as accomplished to date.
Lastly there is the interest of applying social mining tools to an academic dataset.


\subsection{Objectives}
% 1-2 pages double spaced
%Define specific goals of research (which part of general problem am I tackling?)
%Expected outcome
The object of this work is to explore the Nuclear Science References database for latent structures in the body of nuclear science literature.
This includes the quantification of areas of growth and contributions over time.

A further goal is to make available all the resources necessary to repeat this analysis.
All work will be online, open sourced, and publicly version controlled. 

\pagebreak
\section{Methodology}
% 5 pages double spaced. Most critical part. ~70 sentences
%Critical. Define plan of action.
%How to tackle problem? Why this way?

%Creating the DB
\subsection{Data Preparation}
The most preliminary step is acquiring the Nuclear Science References from the National Nuclear Data Center.
A full database dump was acquired on January 29th 2014.
This dump of the NSR data will hereinafter be referred to as if it were the complete NSR database.
All efforts will be taken to ensure the research procedures can very easily be extended and repeated on new NSR data.
This is discussed in further detail in section \ref{sustainability}.

%Data Cleaning
Before the data can be imported into MongoDB it must be parsed into a JSON format.
The NSR data is provided in a custom EXCHANGE format. \citep{winchell2007nuclear}
An example of the raw data for a single paper can be seen in figure !!! %%%%ADD FIGURE
A series of simple search and replace commands using regular expressions can be applied to transform the data into a valid JSON structures.
The JSON structure for the entry in figure \ref{exNSR} is shown as an appendix. %Why JSON?

In order to produce a good JSON schema, thought must be given to the data representation.
Consideration should be given to the types of queries that will be made on the data.
For example the author field is likely be represented as an array of strings, which each unique author being a separate element in the array.
This attaches information about how many authors an entry has to our schema.

\begin{figure}[!hb] \label{exNSR}
\begin{lstlisting}[language=json,firstnumber=1]
<KEYNO   >1988AB01                                                              &
<HISTORY >A19880309 M19880315                                                   &
<CODEN   >JOUR PRVCA 37 401                                                     &
<REFRENCE>Phys.Rev. C37, 401 (1988)                                             &
<AUTHORS >A.Abzouzi, M.S.Antony                                                 &
<TITLE   >Calculation of Energy Levels of {+232}Th,{+232}{+-}{+238}U for K(|p) =&
 0{++} Ground State Bands                                                       &
<KEYWORDS>NUCLEAR STRUCTURE {+232}Th,{+232},{+234},{+236},{+238}U; calculated le&
vels,band features. Semi-empirical formalism.                                   &
<SELECTRS>N:232TH;A. N:232U;A. N:234U;A. N:236U;A. N:238U;A. C:OTHER;A.         &
<DOI     >10.1103/PhysRevC.37.401                                               &
\end{lstlisting} \end{figure}

%Data Representation
The optimal schema for the SELECTRS field is not initially obvious.
The current schema has SELECTRS parsed into a 3 dimensional array.
Each 'selector' has a type, value, and a link variable.
The following types are valid:
\begin{quote}
N, T, P, G, R, S, M, D, C, X, A, or Z, which stand for nuclide, target, parent, daughter, reaction, subject, measured, deduced, calculated, other subject, mass range, and charge range, respectively.
\end{quote}
The value changes based on the type. The link variable is used to tie together multiple selectors.

\subsection{Data Summarization}
%Data Summarization
Data summarization is necessary to begin to understand our data.
Preliminary visualizations help in this task.
A histogram of database entries per year is shown in figure \ref{fig:NSRyearly}.
This very quickly demonstrates that the majority of documents in the NSR were published in the last 50 years.

\begin{figure}[!hb]
    \label{fig:NSRyearly}
    \centering
    \includegraphics[width=\linewidth]{NSRyearly.pdf}
    \caption{NSR Database documents per year from 1896 to 2014.}
\end{figure}

%Form data mining problems, questions, queries
%Interpretation

\subsection{Classification and Cluster Analysis}
Classification and clustering are related approaches to organizing data elements into groups for further analysis.
Classification is the process of deciding what group a particular datum should most optimally belong to.
Clustering is the grouping of multiple data points such that those belonging to a group are more similar in some manner than those outside of that group.

Clustering can be broken into two main groups, hierarchical and partitional.
Both groups have applications in this study.
The citation structure or authorship of the literature is likely to be hierarchical in nature.
Partitional clustering should prove useful in determining the sub genres and fields of study within the body of work.

Various aspects of the data will be clustered with differing techniques based on the data type.
The methodology for a basic clustering technique, K-means clustering is discussed.

\subsubsection{K-means Clustering}
K-means clustering is a cluster analysis technique that can group data objects in $K$ clusters based on minimizing their distances with respect to cluster centroids.
K-means is a partitional clustering algorithm.

Say we have a finite set of objects,  $X = {x_1, x_2, ..., x_n}$ where each is a data object in $d$ dimensions.
We can create $k$ clusters $C = {c_1, c_2, ..., c_k}$ where $k <= n$.
The process starts by randomly choosing $k$ points, ${x_1, ..., x_k}$ to be the centroids of a cluster.
Iterate over each object $x$ and assign it to a cluster $c$ based on the minimization of some parameter, for now, Euclidean distance.
The new centroids are now computed and the process is repeated until cluster stability is achieved.
The goal is to minimize the total sum of squared errors between the centroids and all objects. 
\begin{equation} \label{kmeans}
J(C) = \sum^K_{k=1} \sum_{x_i \in c_k} \left| x_i - \mu_k \right| ^2
\end{equation}

\begin{figure}[!hb]
    \centering
    \includegraphics[width=\linewidth]{../literatureReview/kmeansJain.pdf}
    \caption{Step by step illustration of K-means algorithm. (a) The initial input data. (b) Three seed points are chosen as the starting 'centroids' and the data points are assigned to the cluster with the closest seed point. (c) (d) The centroids of the new clusters are calculated, and data labels updated; (e) the iteration stops when the clusters converge.}
    \label{fig:K-Means-Jain}
\end{figure}

Three parameters for K-means must be specified initially, the number of clusters, initial centroid guesses, and the distance metric.
The metric is the function on a space that describes how two points differ from one another, i.e. distance.
Euclidean distance is typically used, leading to ball or sphere shaped clusters. \citep{Jain2010651}

The chosen number of clusters has a huge impact on the data partitions.
Some heuristics exist to aid in determining an optimal $k$. \citep{tibshirani2001estimating}
However, in practice, K-means is normally run multiple times with varying $k$ values and the best is selected by a domain expert.



\subsection{Text Mining}
Text mining is an area of analysis that focuses on extracting useful information from unstructured plain text data.
The data to analyze is often natural language text written by humans.
Examples of such data include user reviews of a product or service, customer feedback comments, emails, forum posts, or even academic journal articles.

A simple goal of text mining could be to summarize the input text.
However, this can be abstracted further to 'numericizing' text data for use in an analysis model.

\subsubsection{Cosine Similarity}
%https://stackoverflow.com/questions/15173225/how-to-calculate-cosine-similarity-given-2-sentence-strings-python
The K-means discussion above mentioned a few types of distance metrics for use in clustering.
When dealing with text we often use the cosine similarity of two documents to compute their 'distance.
We build a term vector for each document to be analyzed, this is a vector describing the amount of occurrences each word has in the document
Say document one, $d1$, is "The quick brown fox jumped over the lazy dog" and document two, $d2$, is "The brown dog jumped over the brown fox".
We create a term vector for each document, as seen in table \ref{termtable}. 
Often in doing this, very common words like 'the' are omitted.
Equations \ref{d1termvector} and \ref{d2termvector} are the term vectors for each document.
The cosine similarity is the dot product of the two documents divided by the product of their magnitudes (\ref{cosinesimilarity}). \citep{huang2008similarity}

\begin{table} \label{termtable}
    \begin{tabular}{llllllll}
    ~  & quick & brown & fox & jumped & over & lazy & dog \\
    d1 & 1     & 1     & 1   & 1      & 1    & 1    & 1   \\
    d2 & 0     & 2     & 1   & 1      & 1    & 0    & 1   \\
    \end{tabular}
\end{table}

\begin{equation} \label{d1termvector}
d1 = \left< 1, 1, 1, 1, 1, 1, 1 \right>
\end{equation}

\begin{equation} \label{d2termvector}
d2 = \left< 0, 2, 1, 1, 1, 0, 1 \right>
\end{equation}

\begin{equation} \label{cosinesimilarity}
\cos (d1,d1) = \frac{d1 \cdot d2 }{\left| d1 \right| \left| d2 \right|} 
\end{equation}

\begin{equation} \label{cosinesimilarity1}
 d1 \cdot d2 = (1)(0) + (1)(2) + (1)(1) + (1)(1) + (1)(1) + (1)(0) + (1)(1) = 6.0 
\end{equation}

\begin{equation} \label{cosinesimilarity2}
 \left| d1 \right| = \left( (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 \right) ^{\frac{1}{2}} =  2.645751311
\end{equation}

\begin{equation} \label{cosinesimilarity3}
 \left| d2 \right| = \left( (0)^2 + (2)^2 + (1)^2 + (1)^2 + (1)^2 + (0)^2 + (1)^2 \right) ^{\frac{1}{2}} =  2.828427125
\end{equation}

\begin{equation} \label{cosinesimilarityF}
\cos (d1,d2) = \frac{d1 \cdot d2 }{\left| d1 \right| \left| d2 \right|} = \frac{6.0}{\left| 2.645751311 \right| \left| 2.828427125 \right|} = 0.8017837257
\end{equation}

As equation \ref{cosinesimilarityF} shows, the two documents are quite similar, and thus have a high cosine similarity.
When $d1 = d2$ the similarity is 1.0.
This technique is a simple way of numercizing text for further mathematical manipulation and treatment.


\subsection{Sustainability} \label{sustainability}
The analysis and visualizations produced for this work will be constantly updated.
New data can be acquired from the NNDC and uploaded to the local database with relative ease.
This is a result of all procedures being scripted for automation.
As previously stated, an objective of this work is to make it easily repeatable and extensible.
All scripts and source code will be open sourced and in a publicly available version control system.


\pagebreak
\section{Equipment and Research Materials}
% 1 pages double spaced
%Demonstrate feasibility

%Computing resources, hosting, database.
The computing resources for this project will not be substantial.
It is likely that all database queries will be able to complete in a timely manner on a MacBook Pro.
The software systems used are scalable, so despite the low computational requirements of this current data set, the software will be capable of handling much larger data sets.

The work will be hosted on equipment owned by the Mathematics and Computing Science department, and housed in the ITSS server room.

\section{Budget}
% 1 pages double spaced
%Breakdown of expected expenses.
The largest expense of this project would be in producing the original database.
Thankfully the researchers at the NNDC have already done so and willing shared a copy.

The computational and hosting requirements for this work, as outlined above, will be met by existing Saint Mary's resources.
All software used in the completion of the work will be open source.

\pagebreak
\section{Timetable}
% 1 pages double spaced
%Expected date of completion for each major step in research project
%Realistic for 2 year project

The following is an outline for the project's schedule.
It is worth noting that this is a part time Masters, so there is a possibility of delaying any unit by a semester.
Furthermore, this research project is two months old at the time of writing.
Any of the future analysis steps may reveal an area that warrants further investigation that consequently extends this timeline.
That said, great effort will be given to not extend the Masters past January 2016.

\begin{table}[hb]
    \begin{tabular}{|p{6cm}|p{10cm}|}
    Time                      & Work                                                                                                               \\
    January 2014              & Acquired Nuclear Science References dataset from NNDC.                                                             \\
    February 2014             & Cleaned and parsed NSR data into JSON schema.                                                                      \\
    March 2014                & Early visualizations produced with d3.js for data summarization.                                                   \\
    April 2014                & Finalize data representation, schema, and server side handling. Determine useful queries for probing 'structures'. \\
    May - August 2014         & Enable searching and varying parameters of queries. Begin clustering and text mining analysis.                     \\
    September - December 2014 & Further explore structures using KDD techniques, cluster analysis, social data mining tools.                       \\
    January - April 2015      & Last topic course in Data Mining. Finalize cluster analysis, and text mining.                                      \\
    May - August 2015         & Finalize thesis writing.                                                                                           \\
    \end{tabular}
\end{table}

\pagebreak
\section{Appendix: JSON Data Structure}


\begin{figure}[!h] \label{exJSON}
\begin{lstlisting}[language=json,firstnumber=1]
{ "_id": "532f835bede9bfc2462f2aaa",
  "keyNumber": {
    "KEYNO": "1988AB01",
    "yearPublication": 1988,
    "HISTORY": "A19880309 M19880315" },
  "CODEN": [ "JOUR", "PRVCA", "37", "401" ],
  "REFRENCE": "Phys.Rev. C37, 401 (1988)",
  "authors": [ "A.Abzouzi", "M.S.Antony" ],
  "TITLE": "Calculation of Energy Levels of {+232}Th,{+232}{+-}{+238}U for K(|p) = 0{++} Ground State Bands",
  "KEYWORDS": "NUCLEAR STRUCTURE {+232}Th,{+232},{+234},{+236},{+238}U; calculated levels,band features. Semi-empirical formalism.",
  "SELECTRS": [
    {
      "paramType": "N",
      "paramValue": "232TH",
      "linkVar": "A"
    }, {
      "paramType": "N",
      "paramValue": "232U",
      "linkVar": "A"
    }, {
      "paramType": "N",
      "paramValue": "234U",
      "linkVar": "A"
    }, {
      "paramType": "N",
      "paramValue": "236U",
      "linkVar": "A"
    }, {
      "paramType": "N",
      "paramValue": "238U",
      "linkVar": "A"
    }, {
      "paramType": "C",
      "paramValue": "OTHER",
      "linkVar": "A"
    }
  ],
  "DOI": "10.1103/PhysRevC.37.401" }
\end{lstlisting} \end{figure}

\end{doublespacing}

%%% That's all folks
\pagebreak
\bibliography{../citationsAPSC.bib}

\end{document}
